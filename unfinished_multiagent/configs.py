# 生成CFG设置
generation_config_dictllm = {
    "temperature": 0,  # 生成温度，值越高，对于同样的问题，给出不同回答的可能性越高
    "top_p": 0.95,
    "top_k": 64,
    "max_output_tokens": 4096,  # 生成的最大token数量，超过此值，会截断
}

# 生成CFG设置
generation_config_dict_power_llm = {
    "temperature": 0.5,  # 生成温度，值越高，对于同样的问题，给出不同回答的可能性越高
    "top_p": 0.95,
    "top_k": 64,
    "max_output_tokens": 8192,  # 生成的最大token数量，超过此值，会截断
}

# 生成CFG设置
generation_config_dict_json_llm = {
    "temperature": 0,  # 生成温度，值越高，对于同样的问题，给出不同回答的可能性越高
    "top_p": 0.95,
    "top_k": 64,
    "max_output_tokens": 8192,  # 生成的最大token数量，超过此值，会截断
    "response_mime_type": "text/plain",  # 应答格式，可替换为text/json
}